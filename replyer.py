"""PFC å›å¤ç”Ÿæˆå™¨æ¨¡å— - æ ¹æ®ä¸åŒè¡ŒåŠ¨ç±»å‹ç”Ÿæˆå›å¤å†…å®¹ (GPL-3.0)"""

import time
from typing import List, Dict, Any, TYPE_CHECKING

from src.common.logger import get_logger
from src.plugin_system.apis import llm_api
from src.config.config import global_config
from .models import ObservationInfo, ConversationInfo
from .shared import PersonalityHelper, get_current_time_str, translate_timestamp, build_goals_string, build_knowledge_string

if TYPE_CHECKING:
    from .plugin import PFCConfig

logger = get_logger("PFC-Replyer")

_INAPPROPRIATE_PATTERNS = ["ä½œä¸ºAI", "ä½œä¸ºä¸€ä¸ªAI", "ä½œä¸ºäººå·¥æ™ºèƒ½", "æˆ‘æ˜¯AI", "æˆ‘æ˜¯ä¸€ä¸ªAI", "æˆ‘æ˜¯äººå·¥æ™ºèƒ½", "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•", "å¯¹ä¸èµ·ï¼Œæˆ‘ä¸èƒ½"]


def check_basic_reply_quality(reply: str, max_length: int = 500) -> tuple[bool, str]:
    if not reply or len(reply.strip()) == 0:
        return False, "å›å¤ä¸ºç©º"
    if len(reply) > max_length:
        return False, "å›å¤è¿‡é•¿"
    for pattern in _INAPPROPRIATE_PATTERNS:
        if pattern in reply:
            return False, f"åŒ…å«ä¸å½“å†…å®¹: {pattern}"
    return True, ""


def check_reply_similarity(reply: str, chat_history: list, threshold: float = 0.8) -> tuple[bool, str]:
    if not chat_history:
        return True, ""
    for msg in reversed(chat_history[-5:]):
        if msg.get("type") == "bot_message":
            content = msg.get("content", "")
            if content == reply:
                return False, "å›å¤å†…å®¹ä¸ä½ ä¸Šä¸€æ¡å‘è¨€å®Œå…¨ç›¸åŒ"
            import difflib
            ratio = difflib.SequenceMatcher(None, reply, content).ratio()
            if ratio > threshold:
                return False, f"å›å¤å†…å®¹ä¸ä½ ä¸Šä¸€æ¡å‘è¨€é«˜åº¦ç›¸ä¼¼ (ç›¸ä¼¼åº¦ {ratio:.2f})"
            break
    return True, ""


PROMPT_DIRECT_REPLY = """{persona_text}

ã€å›å¤é£æ ¼è¦æ±‚ã€‘
{reply_style}

ã€å½“å‰æ—¶é—´ã€‘
{current_time_str}

ç°åœ¨ä½ åœ¨å‚ä¸ä¸€åœºQQç§èŠï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸€æ¡å›å¤ï¼š

å½“å‰å¯¹è¯ç›®æ ‡ï¼š{goals_str}

{knowledge_info_str}
{tool_info_str}

æœ€è¿‘çš„èŠå¤©è®°å½•ï¼š
{chat_history_text}

è¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯å›å¤å¯¹æ–¹ã€‚è¦æ±‚ï¼šç¬¦åˆå¯¹è¯ç›®æ ‡å’Œä½ çš„æ€§æ ¼ç‰¹å¾ï¼Œé€šä¿—æ˜“æ‡‚ï¼Œè‡ªç„¶æµç•…ï¼Œç®€çŸ­ï¼ˆé€šå¸¸20å­—ä»¥å†…ï¼‰ã€‚
è¯·ç›´æ¥è¾“å‡ºå›å¤å†…å®¹ï¼Œä¸éœ€è¦ä»»ä½•é¢å¤–æ ¼å¼ã€‚"""

PROMPT_SEND_NEW_MESSAGE = """{persona_text}

ã€å›å¤é£æ ¼è¦æ±‚ã€‘
{reply_style}

ã€å½“å‰æ—¶é—´ã€‘
{current_time_str}

ç°åœ¨ä½ åœ¨å‚ä¸ä¸€åœºQQç§èŠï¼Œ**åˆšåˆšä½ å·²ç»å‘é€äº†ä¸€æ¡æˆ–å¤šæ¡æ¶ˆæ¯**ï¼Œç°åœ¨è¯·å†å‘ä¸€æ¡æ–°æ¶ˆæ¯ï¼š

å½“å‰å¯¹è¯ç›®æ ‡ï¼š{goals_str}

{knowledge_info_str}
{tool_info_str}

æœ€è¿‘çš„èŠå¤©è®°å½•ï¼š
{chat_history_text}

è¯·ç»§ç»­å‘ä¸€æ¡æ–°æ¶ˆæ¯ï¼ˆè¡¥å……ã€æ·±å…¥è¯é¢˜æˆ–è¿½é—®ï¼‰ã€‚è¦æ±‚ï¼šç¬¦åˆå¯¹è¯ç›®æ ‡ï¼Œä¸ä¹‹å‰æ¶ˆæ¯è‡ªç„¶è¡”æ¥ï¼Œç®€çŸ­ï¼ˆé€šå¸¸20å­—ä»¥å†…ï¼‰ã€‚
è¯·ç›´æ¥è¾“å‡ºå›å¤å†…å®¹ï¼Œä¸éœ€è¦ä»»ä½•é¢å¤–æ ¼å¼ã€‚"""

PROMPT_FAREWELL = """{persona_text}

ã€å›å¤é£æ ¼è¦æ±‚ã€‘
{reply_style}

ã€å½“å‰æ—¶é—´ã€‘
{current_time_str}

ä½ åœ¨å‚ä¸ä¸€åœº QQ ç§èŠï¼Œç°åœ¨å¯¹è¯ä¼¼ä¹å·²ç»ç»“æŸï¼Œä½ å†³å®šå†å‘ä¸€æ¡æœ€åçš„æ¶ˆæ¯æ¥åœ†æ»¡ç»“æŸã€‚

æœ€è¿‘çš„èŠå¤©è®°å½•ï¼š
{chat_history_text}

è¯·æ„æ€ä¸€æ¡ç®€çŸ­ã€è‡ªç„¶ã€ç¬¦åˆä½ äººè®¾çš„å‘Šåˆ«æ¶ˆæ¯ã€‚
è¯·ç›´æ¥è¾“å‡ºæœ€ç»ˆçš„å‘Šåˆ«æ¶ˆæ¯å†…å®¹ï¼Œä¸éœ€è¦ä»»ä½•é¢å¤–æ ¼å¼ã€‚"""


class ReplyGenerator:
    """å›å¤ç”Ÿæˆå™¨"""

    def __init__(self, session, user_name: str):
        from .plugin import get_config
        from .session import PFCSession
        self.session: PFCSession = session
        self.user_name = user_name
        self.config = get_config()
        self._personality_helper = PersonalityHelper(user_name)
        self.bot_name = self._personality_helper.bot_name

    async def generate(self, action_type: str) -> str:
        prompt_params = await self._build_prompt_params(self.session.observation_info, self.session.conversation_info)
        prompt_template = {"send_new_message": PROMPT_SEND_NEW_MESSAGE, "say_goodbye": PROMPT_FAREWELL}.get(action_type, PROMPT_DIRECT_REPLY)
        prompt = prompt_template.format(**prompt_params)

        try:
            models = llm_api.get_available_models()
            model_name = "replyer_private" if self.config.prompt.inject_system_prompt else "utils"
            model_config = models.get(model_name)
            if not model_config:
                return ""

            success, response, _, _ = await llm_api.generate_with_model(
                prompt=prompt, model_config=model_config, request_type="pfc.reply_generation")

            if not success or not response:
                return ""
            return self._clean_response(response)
        except Exception as e:
            logger.error(f"[ç§èŠ][{self.user_name}]ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {e}")
            return ""

    async def _build_prompt_params(self, observation_info: ObservationInfo, conversation_info: ConversationInfo) -> Dict[str, str]:
        personality_info = await self._personality_helper.get_personality_info()
        goals_str = build_goals_string(conversation_info.goal_list)
        knowledge_info_str = build_knowledge_string(getattr(conversation_info, 'knowledge_list', None))
        chat_history_text = await self._build_chat_history_text(observation_info)
        tool_info_str = await self._build_tool_info(chat_history_text, observation_info)
        
        # æ·»åŠ ä¼šè¯ä¸­çš„å·¥å…·ç»“æœ
        tool_results_str = self._build_tool_results_string(conversation_info)
        if tool_results_str:
            tool_info_str = f"{tool_info_str}\n\n{tool_results_str}" if tool_info_str else tool_results_str

        return {
            "persona_text": personality_info,
            "goals_str": goals_str,
            "knowledge_info_str": knowledge_info_str,
            "tool_info_str": tool_info_str,
            "chat_history_text": chat_history_text,
            "reply_style": self._personality_helper.get_reply_style(),
            "current_time_str": get_current_time_str(),
        }

    def _build_tool_results_string(self, conversation_info: ConversationInfo) -> str:
        """æ„å»ºå·¥å…·ç»“æœå­—ç¬¦ä¸²"""
        tool_results = getattr(conversation_info, 'tool_results', None)
        if not tool_results:
            return ""
        
        # åªæ˜¾ç¤ºæœ€è¿‘çš„å·¥å…·ç»“æœ
        recent_results = tool_results[-5:]
        if not recent_results:
            return ""
        
        parts = ["### ğŸ”§ æœ€è¿‘çš„å·¥å…·æ‰§è¡Œç»“æœ"]
        for result in recent_results:
            tool_name = result.get("tool_name", "unknown")
            content = result.get("content", "")
            # æˆªæ–­è¿‡é•¿çš„å†…å®¹
            if len(content) > 300:
                content = content[:300] + "..."
            parts.append(f"- **{tool_name}**: {content}")
        
        return "\n".join(parts)

    async def _build_tool_info(self, chat_history_text: str, observation_info: ObservationInfo) -> str:
        if not self.config.tool.enabled or not self.config.tool.enable_in_replyer:
            return ""
        try:
            from .context_builder import PFCContextBuilder
            builder = PFCContextBuilder(self.session.stream_id, self.config)
            target_message = observation_info.chat_history[-1].get("content", "") if observation_info.chat_history else ""
            return await builder.build_tool_info(chat_history_text, self.user_name, target_message, True)
        except Exception as e:
            logger.error(f"[ç§èŠ][{self.user_name}] æ„å»ºå·¥å…·ä¿¡æ¯å¤±è´¥: {e}")
            return ""

    async def _build_chat_history_text(self, observation_info: ObservationInfo) -> str:
        chat_history_text = observation_info.chat_history_str
        if observation_info.new_messages_count > 0 and observation_info.unprocessed_messages:
            new_messages_str = self._format_messages(observation_info.unprocessed_messages)
            if new_messages_str:
                chat_history_text += f"\n--- ä»¥ä¸‹æ˜¯ {observation_info.new_messages_count} æ¡æ–°æ¶ˆæ¯ ---\n{new_messages_str}"
        return chat_history_text or "è¿˜æ²¡æœ‰èŠå¤©è®°å½•ã€‚"

    def _format_messages(self, messages: List[Dict[str, Any]], timestamp_mode: str = "relative") -> str:
        if not messages:
            return ""
        formatted_blocks = []
        for msg in messages:
            sender = msg.get("sender", {})
            sender_name = sender.get("nickname", "æœªçŸ¥ç”¨æˆ·")
            user_name = msg.get("user_name", sender_name)
            content = msg.get("processed_plain_text", msg.get("content", ""))
            timestamp = msg.get("time", time.time())
            user_id = sender.get("user_id", msg.get("user_id", ""))
            if global_config and global_config.bot and str(user_id) == str(global_config.bot.qq_account):
                sender_name = f"{self.bot_name}(ä½ )"
            else:
                sender_name = user_name or sender_name
            readable_time = translate_timestamp(timestamp, mode=timestamp_mode)
            formatted_blocks.append(f"{readable_time} {sender_name} è¯´:")
            if content:
                stripped = content.strip()
                if stripped:
                    if stripped.endswith("ã€‚"):
                        stripped = stripped[:-1]
                    formatted_blocks.append(f"{stripped};")
            formatted_blocks.append("")
        return "\n".join(formatted_blocks).strip()

    async def check_reply(self, reply: str, goal: str) -> tuple[bool, str, bool]:
        valid, reason = check_basic_reply_quality(reply)
        if not valid:
            return False, reason, True
        valid, reason = check_reply_similarity(reply, self.session.observation_info.chat_history)
        if not valid:
            return False, reason, True
        return True, "å›å¤æ£€æŸ¥é€šè¿‡", False

    def _clean_response(self, response: str) -> str:
        if not response:
            return ""
        content = response.strip()
        if (content.startswith('"') and content.endswith('"')) or (content.startswith("'") and content.endswith("'")):
            content = content[1:-1]
        for prefix in ["å›å¤ï¼š", "å›å¤:", "Reply:", "reply:", "æ¶ˆæ¯ï¼š", "æ¶ˆæ¯:", "Message:", "message:"]:
            if content.startswith(prefix):
                content = content[len(prefix):].strip()
                break
        return content


class ReplyChecker:
    """å›å¤æ£€æŸ¥å™¨"""

    def __init__(self, stream_id: str, private_name: str, config: "PFCConfig"):
        self.stream_id = stream_id
        self.private_name = private_name
        self.config = config
        self.checker_config = config.reply_checker
        self.max_retries = self.checker_config.max_retries

    async def check(self, reply: str, goal: str, chat_history: List[Dict[str, Any]],
                   chat_history_str: str, retry_count: int = 0) -> tuple[bool, str, bool]:
        if not self.checker_config.enabled:
            return True, "æ£€æŸ¥å™¨å·²ç¦ç”¨ï¼Œç›´æ¥é€šè¿‡", False

        valid, reason = check_basic_reply_quality(reply)
        if not valid:
            return False, reason, True

        valid, reason = check_reply_similarity(reply, chat_history, self.checker_config.similarity_threshold)
        if not valid:
            return False, f"è¢«é€»è¾‘æ£€æŸ¥æ‹’ç»ï¼š{reason}", True

        if self.checker_config.use_llm_check:
            return await self._llm_check(reply, goal, chat_history_str, retry_count)

        if retry_count >= self.max_retries:
            return True, "é‡è¯•æ¬¡æ•°è¿‡å¤šï¼Œæ¥å—å½“å‰å›å¤", False
        return True, "å›å¤æ£€æŸ¥é€šè¿‡", False

    async def _llm_check(self, reply: str, goal: str, chat_history_str: str, retry_count: int) -> tuple[bool, str, bool]:
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªèŠå¤©é€»è¾‘æ£€æŸ¥å™¨ï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹å›å¤æ˜¯å¦åˆé€‚ï¼š

å½“å‰å¯¹è¯ç›®æ ‡ï¼š{goal}
æœ€æ–°çš„å¯¹è¯è®°å½•ï¼š
{chat_history_str}

å¾…æ£€æŸ¥çš„æ¶ˆæ¯ï¼š
{reply}

è¯·æ£€æŸ¥ï¼š1.æ˜¯å¦ç¬¦åˆç›®æ ‡ 2.æ˜¯å¦ä¸è®°å½•ä¸€è‡´ 3.æ˜¯å¦é‡å¤å‘è¨€ 4.æ˜¯å¦è¿è§„ 5.æ˜¯å¦é€šä¿—æ˜“æ‡‚ 6.æ˜¯å¦è¿‡äºå†—é•¿

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼š{{"suitable": true/false, "reason": "åŸå› ", "need_replan": true/false}}"""

        try:
            models = llm_api.get_available_models()
            checker_config = models.get("utils")
            if not checker_config:
                return True, "LLM æ£€æŸ¥è·³è¿‡ï¼ˆæ— æ¨¡å‹é…ç½®ï¼‰", False

            success, content, _, _ = await llm_api.generate_with_model(
                prompt=prompt, model_config=checker_config, request_type="pfc.reply_check")

            if not success or not content:
                return True, "LLM æ£€æŸ¥è·³è¿‡ï¼ˆè°ƒç”¨å¤±è´¥ï¼‰", False
            return self._parse_llm_response(content, retry_count)
        except Exception as e:
            logger.error(f"[ç§èŠ][{self.private_name}]LLM æ£€æŸ¥æ—¶å‡ºé”™: {e}")
            return False if retry_count >= self.max_retries else False, "æ£€æŸ¥è¿‡ç¨‹å‡ºé”™", retry_count >= self.max_retries

    def _parse_llm_response(self, content: str, retry_count: int) -> tuple[bool, str, bool]:
        import json
        import re
        content = content.strip()
        try:
            result = json.loads(content)
        except json.JSONDecodeError:
            json_match = re.search(r"\{[^{}]*\}", content)
            if json_match:
                try:
                    result = json.loads(json_match.group())
                except json.JSONDecodeError:
                    return self._fallback_parse(content, retry_count)
            else:
                return self._fallback_parse(content, retry_count)

        suitable = result.get("suitable")
        reason = result.get("reason", "æœªæä¾›åŸå› ")
        need_replan = result.get("need_replan", False)

        if isinstance(suitable, str):
            suitable = suitable.lower() == "true"
        if suitable is None:
            suitable = "ä¸åˆé€‚" not in reason.lower() and "è¿è§„" not in reason.lower()

        if not suitable:
            if retry_count >= self.max_retries:
                return False, f"å¤šæ¬¡é‡è¯•åä»ä¸åˆé€‚: {reason}", True
            return False, reason, False
        return suitable, reason, need_replan

    def _fallback_parse(self, content: str, retry_count: int) -> tuple[bool, str, bool]:
        is_suitable = "ä¸åˆé€‚" not in content.lower() and "è¿è§„" not in content.lower()
        reason = content[:100] if content else "æ— æ³•è§£æå“åº”"
        need_replan = "é‡æ–°è§„åˆ’" in content.lower() or "ç›®æ ‡ä¸é€‚åˆ" in content.lower()
        return is_suitable, reason, need_replan